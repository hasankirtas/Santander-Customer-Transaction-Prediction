{
 "cells": [
  {
   "cell_type": "raw",
   "id": "877f0fc6-d87c-463f-be50-fbdaddf575e2",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # For model saving\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('../data/processed/undersampled_train_data.csv')\n",
    "\n",
    "# Define features\n",
    "rf_features = [\n",
    "    \"var_81\", \"var_146\", \"var_12\", \"var_76\", \"var_174\", \"var_34\", \"var_21\", \"var_165\",\n",
    "    \"var_109\", \"var_44\", \"var_166\", \"var_198\", \"var_192\", \"var_148\", \"var_33\", \"var_80\",\n",
    "    \"var_169\", \"var_115\", \"var_92\", \"var_149\", \"var_154\", \"var_121\", \"var_107\", \"var_127\",\n",
    "    \"var_122\", \"var_172\", \"var_177\", \"var_36\", \"var_108\", \"var_75\", \"var_188\", \"var_123\",\n",
    "    \"var_87\", \"var_197\", \"var_86\", \"var_93\", \"var_31\"\n",
    "]\n",
    "xgb_features = [\n",
    "    \"var_6\", \"var_53\", \"var_26\", \"var_110\", \"var_99\", \"var_190\", \"var_133\", \"var_22\",\n",
    "    \"var_179\", \"var_2\", \"var_94\", \"var_40\", \"var_78\", \"var_173\", \"var_184\", \"var_170\",\n",
    "    \"var_0\", \"var_1\", \"var_191\", \"var_67\", \"var_118\", \"var_147\", \"var_18\", \"var_164\",\n",
    "    \"var_89\", \"var_35\", \"var_48\", \"var_95\", \"var_199\", \"var_155\", \"var_32\", \"var_5\",\n",
    "    \"var_91\", \"var_90\", \"var_71\", \"var_157\", \"var_162\", \"var_130\", \"var_135\", \"var_52\"\n",
    "]\n",
    "\n",
    "X_rf = train_data[rf_features]\n",
    "X_xgb = train_data[xgb_features]\n",
    "y = train_data['target']\n",
    "\n",
    "# Scale data\n",
    "scaler_rf = StandardScaler()\n",
    "scaler_xgb = StandardScaler()\n",
    "X_rf_scaled = scaler_rf.fit_transform(X_rf)\n",
    "X_xgb_scaled = scaler_xgb.fit_transform(X_xgb)\n",
    "\n",
    "# Stratified K-Fold settings\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Compute class weight for imbalance\n",
    "class_0_count = sum(y == 0)\n",
    "class_1_count = sum(y == 1)\n",
    "scale_pos_weight = class_0_count / class_1_count\n",
    "\n",
    "# Store model scores\n",
    "rf_scores, xgb_scores, voting_scores = [], [], []\n",
    "\n",
    "# Cross-validation\n",
    "for train_idx, test_idx in skf.split(X_rf_scaled, y):\n",
    "    X_rf_train, X_rf_test = X_rf_scaled[train_idx], X_rf_scaled[test_idx]\n",
    "    X_xgb_train, X_xgb_test = X_xgb_scaled[train_idx], X_xgb_scaled[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Random Forest Model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=1500, max_depth=12, min_samples_split=5, min_samples_leaf=3,\n",
    "        max_features='sqrt', bootstrap=True, n_jobs=-1, random_state=42,\n",
    "        class_weight={0: 1, 1: scale_pos_weight}\n",
    "    )\n",
    "    rf_model.fit(X_rf_train, y_train)\n",
    "    rf_scores.append(accuracy_score(y_test, rf_model.predict(X_rf_test)))\n",
    "\n",
    "    # XGBoost Model\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=1500, learning_rate=0.01, max_depth=10, colsample_bytree=0.75,\n",
    "        subsample=0.75, gamma=0.3, reg_alpha=0.1, reg_lambda=0.8,\n",
    "        scale_pos_weight=scale_pos_weight, n_jobs=-1, random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_xgb_train, y_train)\n",
    "    xgb_scores.append(accuracy_score(y_test, xgb_model.predict(X_xgb_test)))\n",
    "\n",
    "    # Soft Voting Model\n",
    "    voting_model = VotingClassifier(\n",
    "        estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='soft'\n",
    "    )\n",
    "    voting_model.fit(np.hstack((X_rf_train, X_xgb_train)), y_train)\n",
    "    voting_scores.append(accuracy_score(y_test, voting_model.predict(np.hstack((X_rf_test, X_xgb_test)))))\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest Accuracy: {np.mean(rf_scores):.4f} ± {np.std(rf_scores):.4f}\")\n",
    "print(f\"XGBoost Accuracy: {np.mean(xgb_scores):.4f} ± {np.std(xgb_scores):.4f}\")\n",
    "print(f\"Soft Voting Accuracy: {np.mean(voting_scores):.4f} ± {np.std(voting_scores):.4f}\")\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_model, \"../models/rf_model.pkl\")\n",
    "joblib.dump(xgb_model, \"../models/xgb_model.pkl\")\n",
    "joblib.dump(voting_model, \"../models/voting_model.pkl\")\n",
    "joblib.dump(scaler_rf, \"../models/scaler_rf.pkl\")  \n",
    "joblib.dump(scaler_xgb, \"../models/scaler_xgb.pkl\")  \n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83106b5e-34b9-490e-abf8-869b4936e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7366 ± 0.0032\n",
      "XGBoost Accuracy: 0.7955 ± 0.0008\n",
      "Random Forest Average Training Time: 269.87 seconds\n",
      "XGBoost Average Training Time: 23.85 seconds\n",
      "Voting model and scalers saved.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib  # For model saving\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('../data/processed/undersampled_train_data.csv')\n",
    "\n",
    "# Define features and target\n",
    "rf_features = [\n",
    "    \"var_81\", \"var_146\", \"var_12\", \"var_76\", \"var_174\", \"var_34\", \"var_21\", \"var_165\",\n",
    "    \"var_109\", \"var_44\", \"var_166\", \"var_198\", \"var_192\", \"var_148\", \"var_33\", \"var_80\",\n",
    "    \"var_169\", \"var_115\", \"var_92\", \"var_149\", \"var_154\", \"var_121\", \"var_107\", \"var_127\",\n",
    "    \"var_122\", \"var_172\", \"var_177\", \"var_36\", \"var_108\", \"var_75\", \"var_188\", \"var_123\",\n",
    "    \"var_87\", \"var_197\", \"var_86\", \"var_93\", \"var_31\"\n",
    "]\n",
    "\n",
    "xgb_features = [\n",
    "    \"var_6\", \"var_53\", \"var_26\", \"var_110\", \"var_99\", \"var_190\", \"var_133\", \"var_22\",\n",
    "    \"var_179\", \"var_2\", \"var_94\", \"var_40\", \"var_78\", \"var_173\", \"var_184\", \"var_170\",\n",
    "    \"var_0\", \"var_1\", \"var_191\", \"var_67\", \"var_118\", \"var_147\", \"var_18\", \"var_164\",\n",
    "    \"var_89\", \"var_35\", \"var_48\", \"var_95\", \"var_199\", \"var_155\", \"var_32\", \"var_5\",\n",
    "    \"var_91\", \"var_90\", \"var_71\", \"var_157\", \"var_162\", \"var_130\", \"var_135\", \"var_52\"\n",
    "]\n",
    "\n",
    "X_rf = train_data[rf_features]\n",
    "X_xgb = train_data[xgb_features]\n",
    "y = train_data['target']\n",
    "\n",
    "# Scale the data\n",
    "scaler_rf = StandardScaler()\n",
    "scaler_xgb = StandardScaler()\n",
    "\n",
    "X_rf_scaled = scaler_rf.fit_transform(X_rf)\n",
    "X_xgb_scaled = scaler_xgb.fit_transform(X_xgb)\n",
    "\n",
    "# Stratified K-Fold Setup\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "class_0_count = sum(y == 0)\n",
    "class_1_count = sum(y == 1)\n",
    "scale_pos_weight = class_0_count / class_1_count\n",
    "\n",
    "rf_scores, xgb_scores = [], []\n",
    "rf_times, xgb_times = [], []  # Lists to store training times\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "for train_idx, test_idx in skf.split(X_rf_scaled, y):\n",
    "    X_rf_train, X_rf_test = X_rf_scaled[train_idx], X_rf_scaled[test_idx]\n",
    "    X_xgb_train, X_xgb_test = X_xgb_scaled[train_idx], X_xgb_scaled[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Random Forest Model\n",
    "    params_rf = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 4,\n",
    "        'min_samples_leaf': 2,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': False,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'class_weight': {0: 1, 1: scale_pos_weight}\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    rf_model = RandomForestClassifier(**params_rf)\n",
    "    rf_model.fit(X_rf_train, y_train)\n",
    "    rf_times.append(time.time() - start_time)  # Save training time\n",
    "    rf_scores.append(accuracy_score(y_test, rf_model.predict(X_rf_test)))\n",
    "\n",
    "    # XGBoost Model\n",
    "    params_xgb = {\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 8,\n",
    "        'colsample_bytree': 0.75,\n",
    "        'subsample': 0.75,\n",
    "        'gamma': 0.3,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    start_time = time.time()\n",
    "    xgb_model = XGBClassifier(**params_xgb)\n",
    "    xgb_model.fit(X_xgb_train, y_train)\n",
    "    xgb_times.append(time.time() - start_time)  # Save training time\n",
    "    xgb_scores.append(accuracy_score(y_test, xgb_model.predict(X_xgb_test)))\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest Accuracy: {np.mean(rf_scores):.4f} ± {np.std(rf_scores):.4f}\")\n",
    "print(f\"XGBoost Accuracy: {np.mean(xgb_scores):.4f} ± {np.std(xgb_scores):.4f}\")\n",
    "\n",
    "# Print training times\n",
    "print(f\"Random Forest Average Training Time: {np.mean(rf_times):.2f} seconds\")\n",
    "print(f\"XGBoost Average Training Time: {np.mean(xgb_times):.2f} seconds\")\n",
    "\n",
    "# Soft Voting Model (Trained on all data at once)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='soft'\n",
    ")\n",
    "voting_model.fit(np.hstack((X_rf_scaled, X_xgb_scaled)), y)\n",
    "\n",
    "# Save the Voting model\n",
    "joblib.dump(voting_model, '../models/voting_model.pkl')\n",
    "\n",
    "# Save the scalers\n",
    "joblib.dump(scaler_rf, '../models/scaler_rf.pkl')\n",
    "joblib.dump(scaler_xgb, '../models/scaler_xgb.pkl')\n",
    "\n",
    "print(\"Voting model and scalers saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
