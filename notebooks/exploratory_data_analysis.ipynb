{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bdf00e-56fe-4105-a939-b16d03b6e881",
   "metadata": {},
   "source": [
    "## An Overview to the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f6fa8-fb02-49ba-9a1b-7da7daf7b480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv(\"../data/raw/train.csv\").copy()\n",
    "\n",
    "# Print basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(train_data.info())\n",
    "\n",
    "# Check column names and data types\n",
    "print(\"Columns and Data Types:\")\n",
    "print(train_data.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Display basic statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(train_data.describe())\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=train_data[\"target\"], palette=\"viridis\")\n",
    "plt.title(\"Target Variable Distribution\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Check the count of the target variable\n",
    "target_counts = train_data[\"target\"].value_counts()\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17206b-d9c5-433c-95bc-16cd363ff517",
   "metadata": {},
   "source": [
    "## Outlier Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93c404-afe5-4daf-9444-6de5e62a7869",
   "metadata": {},
   "source": [
    "The numbers of outliers does not pose any risk to the models those will be created. Also, to minimize potential risks, robust models such as ensemble models will be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec72bc-8f27-4aae-b39b-6b5c48093b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting independent variables from numeric data\n",
    "train_data_numeric = train_data.drop(columns=['ID_code', 'target'])\n",
    "\n",
    "# Detecting outliers using the IQR method\n",
    "def detect_outliers(train_data):\n",
    "    outliers = {}\n",
    "    for column in train_data.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = train_data[column].quantile(0.25)\n",
    "        Q3 = train_data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Indices of outlier values\n",
    "        outliers[column] = train_data[(train_data[column] < lower_bound) | (train_data[column] > upper_bound)].index.tolist()\n",
    "    return outliers\n",
    "\n",
    "# Getting the count of outliers\n",
    "outliers = detect_outliers(train_data_numeric)\n",
    "\n",
    "# Printing the number of outliers in each column\n",
    "for column, indices in outliers.items():\n",
    "    print(f\"{column}: {len(indices)} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0f74d-aa39-443f-81b2-b884154289cf",
   "metadata": {},
   "source": [
    "## Skewness and Kurtosis Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07833662-9dbc-4e9c-9215-3e3367487d63",
   "metadata": {},
   "source": [
    "According to the analysis result, there is no striking issues in any of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8ab85-16bf-4703-b271-97b4aca75f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numeric_columns = train_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Perform normality tests and statistical analyses for each numerical column\n",
    "# Report the most important outputs\n",
    "for column in numeric_columns:\n",
    "    print(f'Analysis for column: {column}')\n",
    "    \n",
    "    column_data = train_data[column]\n",
    "\n",
    "    # Shapiro-Wilk Test (Normality Test)\n",
    "    shapiro_stat, shapiro_p_value = stats.shapiro(column_data)\n",
    "    print(f'Shapiro-Wilk Test for {column}: p-value={shapiro_p_value}')\n",
    "    \n",
    "    # Skewness and Kurtosis\n",
    "    skewness = column_data.skew()\n",
    "    kurtosis = column_data.kurtosis()\n",
    "    \n",
    "    print(f'Skewness for {column}: {skewness}')\n",
    "    print(f'Kurtosis for {column}: {kurtosis}')\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feceeda4-05e9-4d3f-8963-291232315f1c",
   "metadata": {},
   "source": [
    "### Density Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc858803-5c6d-4597-9f5b-79406d0664c8",
   "metadata": {},
   "source": [
    "#### Determining Features\n",
    "\n",
    "var_0, var_1, var_2, var_6, var_9, var_12, var_13, var_21, var_22, var_26, var_40, var_44, var_53, var_75, var_76, var_80, var_81, var_86, var_89, var_93, var_94, var_95, var_99, var_108, var_109, var_110, var_123, var_139, var_141, var_148, var_164, var_165, var_166, var_177, var_191, var_198: These features are first-order determining features, exhibiting the most prominent differences between classes 0 and 1. Upon examining the density plots, it is observed that the distributions of these features are significantly separated between the classes. These features can be used to enhance the model's classification performance and should be prioritized in feature engineering efforts.\n",
    "\n",
    "var_5, var_16, var_18, var_24, var_28, var_32, var_33, var_34, var_35, var_36, var_41,var_43, var_52, var_55, var_56, var_58, var_60, var_62, var_67, var_78, var_92, var_101, var_106, var_115, var_119,var_127, var_131, var_133, var_137, var_145, var_146, var_149, var_150, var_151,var_154, var_157, var_163, var_169, var_174, var_179, var_180, var_184, var_188, var_190, var_195, var_196, var_197: While these features also show distinct differences between the classes, they are not as decisive as the features mentioned above. These features can be considered second-order determining features. Nevertheless, these features should also be evaluated to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62781e-9cef-488e-b31f-94d4991e604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(df1, df2, label1, label2, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(10,10,figsize=(18,22))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(10,10,i)\n",
    "        sns.distplot(df1[feature], hist=False,label=label1)\n",
    "        sns.distplot(df2[feature], hist=False,label=label2)\n",
    "        plt.xlabel(feature, fontsize=9)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1310bae-2dd4-4c93-a462-d972e6b97a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = train_data.loc[train_data['target'] == 0]\n",
    "t1 = train_data.loc[train_data['target'] == 1]\n",
    "features = train_data.columns.values[2:102]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7e906-d30c-468f-9912-459ffbec207f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = train_data.columns.values[102:202]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3176e-6ba2-47ae-a53a-c719dd440179",
   "metadata": {},
   "source": [
    "## Feature Selection for Each Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba48b5-b91d-424a-9265-77424382a089",
   "metadata": {},
   "source": [
    "In this part performs model-based feature selection. Lasso regression, Random Forest, and XGBoost rank the features based on their importance in predicting the target variable. For each model, importance rankings are generated for class 0, class 1, and overall. Additionally, SHAP analysis is used to visualize the contribution of features to the predictions of the XGBoost model. These processes help us understand which features have a greater impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c03098-315e-4dbb-b6fb-7706879b782e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exclude 'ID_code' and 'target' columns\n",
    "X = train_data.drop(columns=['ID_code', 'target'])\n",
    "y = train_data['target']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Lasso (L1) Regression Feature Selection\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, max_iter=2000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_coefficients = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Lasso_Coefficient\": lasso.coef_[0]\n",
    "}).sort_values(by=\"Lasso_Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n=== Lasso Top 30 Features ===\")\n",
    "print(lasso_coefficients.head(30))\n",
    "\n",
    "# Lasso (L1) Regression Feature Selection",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, max_iter=2000)",
    "lasso.fit(X_train_scaled, y_train)",
    "",
    "lasso_coefficients = pd.DataFrame({",
    "    \"Feature\": X.columns,",
    "    \"Lasso_Coefficient\": lasso.coef_[0]",
    "}).sort_values(by=\"Lasso_Coefficient\", key=abs, ascending=False)",
    "",
    "lasso_zeros = lasso_coefficients[lasso_coefficients[\"Lasso_Coefficient\"] < 0]",
    "lasso_zeros = lasso_zeros.sort_values(by=\"Lasso_Coefficient\", ascending=True)",
    "",
    "lasso_ones = lasso_coefficients[lasso_coefficients[\"Lasso_Coefficient\"] > 0]",
    "lasso_ones = lasso_ones.sort_values(by=\"Lasso_Coefficient\", ascending=False)",
    "",
    "print(\"\\n=== Lasso (L1) Most Important Features ===\")",
    "print(\"\\nMost important 40 features for class 0:\")",
    "print(lasso_zeros.head(40))",
    "print(\"\\nMost important 40 features for class 1:\")",
    "print(lasso_ones.head(40))"
    "# Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance_rf = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== Random Forest Top 60 Features ===\")\n",
    "print(feature_importance_rf.head(60))\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance_xgb = pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== XGBoost Top 60 Features ===\")\n",
    "print(feature_importance_xgb.head(60))\n",
    "\n",
    "# SHAP Analysis\n",
    "explainer = shap.Explainer(xgb)\n",
    "shap_values = explainer(X_train_scaled)\n",
    "shap.summary_plot(shap_values, X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55393e7b-7dd3-436d-a6c5-5f54426c92b5",
   "metadata": {},
   "source": [
    "## Final Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a672b8-5b4b-481b-85bc-bc3d52a6809a",
   "metadata": {},
   "source": [
    "These features were selected based on the numerical distribution differences of the independent variables between the target classes (class 0 and class 1). In the selection process, features that showed a more distinct separation between the classes and could enhance the model's performance were prioritized. These features are valuable variables that exhibit statistical differences among the independent variables and improve the model's ability to make accurate predictions. Additionally, feature importance analysis using models such as Lasso regression, Random Forest, and XGBoost further supported these selections. According to the outputs of these models, the most decisive features were selected with higher importance and used to optimize the model's classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc69c35-c0e8-48f3-a2c2-f9481b655614",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_important_features = [\n",
    "    'var_81', 'var_12', 'var_6', 'var_26', 'var_53', 'var_110', 'var_146',\n",
    "    'var_174', 'var_109', 'var_22', 'var_166', 'var_99', 'var_80', 'var_21', 'var_76',\n",
    "    'var_133', 'var_2', 'var_198', 'var_165', 'var_190', 'var_179', 'var_0',\n",
    "    'var_78', 'var_148', 'var_40', 'var_44', 'var_34', 'var_170', 'var_94',\n",
    "    'var_92', 'var_164', 'var_115', 'var_33', 'var_67', 'var_121', 'var_184', 'var_177',\n",
    "    'var_149', 'var_108', 'var_18', 'var_154', 'var_169', 'var_192', 'var_173', 'var_191',\n",
    "    'var_127', 'var_75', 'var_118', 'var_122', 'var_91', 'var_107', 'var_123',\n",
    "    'var_56', 'var_155', 'var_147', 'var_86', 'var_95', 'var_172', 'var_162',\n",
    "    \"var_36\", \"var_188\", \"var_87\", \"var_197\", \"var_93\", \"var_31\", \"var_89\",\n",
    "    \"var_35\", \"var_48\", \"var_199\", \"var_32\", \"var_90\", \"var_71\", \n",
    "    \"var_157\", \"var_130\", \"var_135\"\n",
    "]\n",
    "\n",
    "class_0_important_features = [\n",
    "    \"var_81\", \"var_146\", \"var_12\", \"var_76\", \"var_174\", \"var_34\", \"var_21\", \"var_165\",\n",
    "    \"var_109\", \"var_44\", \"var_166\", \"var_198\", \"var_192\", \"var_148\", \"var_33\", \"var_80\",\n",
    "    \"var_169\", \"var_115\", \"var_92\", \"var_149\", \"var_154\", \"var_121\", \"var_107\", \"var_127\",\n",
    "    \"var_122\", \"var_172\", \"var_177\", \"var_36\", \"var_108\", \"var_75\", \"var_188\", \"var_123\",\n",
    "    \"var_87\", \"var_197\", \"var_86\", \"var_93\", \"var_31\"\n",
    "]  \n",
    "class_1_important_features = [\n",
    "    \"var_6\", \"var_53\", \"var_26\", \"var_110\", \"var_99\", \"var_190\", \"var_133\", \"var_22\",\n",
    "    \"var_179\", \"var_2\", \"var_94\", \"var_40\", \"var_78\", \"var_173\", \"var_184\", \"var_170\",\n",
    "    \"var_0\", \"var_1\", \"var_191\", \"var_67\", \"var_118\", \"var_147\", \"var_18\", \"var_164\",\n",
    "    \"var_89\", \"var_35\", \"var_48\", \"var_95\", \"var_199\", \"var_155\", \"var_32\", \"var_5\",\n",
    "    \"var_91\", \"var_90\", \"var_71\", \"var_157\", \"var_162\", \"var_130\", \"var_135\", \"var_52\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d17a9a-2865-4b6b-a54e-96213ad6e68d",
   "metadata": {},
   "source": [
    "## Resampling the Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bba9c-6d76-4ba9-ba0b-b100ed6fbdf2",
   "metadata": {},
   "source": [
    "In the code, undersampling is applied to reduce the majority class (class 0) while maintaining the distribution of the minority class (class 1). Initially, Tomek Links is used to clean the majority class by removing noisy examples, and then the majority class is reduced by 20%. This results in a more balanced dataset, with the minority class's distribution preserved and the majority class slightly downsampled. The final resampled data is saved for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be5fbe-a329-4573-9918-ed15c8e32a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select features and target variable\n",
    "X = train_data[overall_important_features]\n",
    "y = train_data['target']\n",
    "id_code_column = train_data['ID_code']  # Preserve ID_code\n",
    "\n",
    "# Apply Tomek Links to remove borderline majority class samples\n",
    "tomek = TomekLinks() ve RandomUnderSampler()\n",
    "X_resampled, y_resampled = tomek.fit_resample(X, y)\n",
    "\n",
    "# Reduce the majority class by 20%\n",
    "majority_class_count = y_resampled.value_counts()[0]\n",
    "target_majority_count = int(majority_class_count * 0.7)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={0: target_majority_count, 1: y_resampled.value_counts()[1]}, random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Save the resampled dataset\n",
    "resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "resampled_data.insert(0, 'ID_code', id_code_column[:len(resampled_data)])\n",
    "resampled_data.insert(1, 'target', y_resampled)\n",
    "\n",
    "resampled_data.to_csv(\"../data/processed/undersampled_train_data.csv\", index=False)\n",
    "\n",
    "print(\"Resampling completed and the dataset is saved!\")\n",
    "print(f\"New class distribution:\\n{y_resampled.value_counts()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
